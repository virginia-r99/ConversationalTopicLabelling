{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter Notebook to, given a set of embeddings, compute their cosine similarity _(in this case, of the answers-ground truth topic label pairs)_.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import walk,rename\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the topic ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the topics (names and descriptions)\n",
    "topics = pd.read_json(\"topics_20news.json\")\n",
    "\n",
    "# Extract the topics' names and define a results dataframe to save similarities\n",
    "results = pd.DataFrame({'topics':topics['name']})\n",
    "# Show topics' names\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two similarities that can be generated:\n",
    "- Similarities of conversational answers\n",
    "- Similarities of QA answers\n",
    "\n",
    "In the following code we show both methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONV. SIMILARITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a. For the conversational models, embedding model,question structure and number of topic words selected, generate the similarity values between the topic ground truth embedding and the conversational answer embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of conversational models\n",
    "#conv_models = [\"blenderbot-400M-distill\",\"DialoGPT-medium\",\"ChatGPT\"]\n",
    "conv_models = [\"bart-tl-all\",\"bart-tl-ng\"]\n",
    "\n",
    "# Define the embedding model name\n",
    "#emb_model = \"all-mpnet-base-v2\"\n",
    "emb_model = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Define the path where the embeddings of the topics are stored\n",
    "emb_topic_path = \"Bart-TL/BartEmbeddings/\"+emb_model+\"/topics/\"\n",
    "\n",
    "# Define the name of the question structure used\n",
    "q = \"q3\"\n",
    "\n",
    "# Define the number of words to be used\n",
    "num = [4,10]\n",
    "# For each number of words\n",
    "for i in num:\n",
    "    # Define the path where the embeddings of the answers are stored\n",
    "    emb_path = \"Bart-TL/BartEmbeddings/\"+emb_model+\"/\"\n",
    "    # Define the path where the similarities are to be stored\n",
    "    sim_path = \"Bart-TL/Similarities/\"\n",
    "    \n",
    "    # For each conversational model\n",
    "    for c in conv_models:\n",
    "        # Array to save the similarities\n",
    "        sims = []\n",
    "        # Define the prefix name of the similarities' files of the conversational model c with i words\n",
    "        #prefix = \"tensor_\"+q+\"_topics_results_\"+str(i)+\"_\"+c+\"_\"\n",
    "        prefix = \"tensor_topics_results_\"+str(i)+\"_\"+c+\"_\"\n",
    "        # For each topic (we have 20 topics)\n",
    "        for j in range(0,20):\n",
    "            # Define the path of the file where the conversational answer tensor for topic j is\n",
    "            t_answ_name = emb_path+prefix+str(j)+\".pt\"\n",
    "            # Define the path of the file where the groud truth tensor for topic j is\n",
    "            t_topic_name = emb_topic_path+\"tensor_topic_\"+str(j)+\".pt\"\n",
    "            # Load the conv. answer and GT tensors\n",
    "            emb_answ = torch.load(t_answ_name)\n",
    "            emb_topic = torch.load(t_topic_name)\n",
    "\n",
    "            # Compute similarity scores of the two embeddings/tensors\n",
    "            cosine_scores = util.pytorch_cos_sim(emb_topic, emb_answ)\n",
    "            # Save the similarity value\n",
    "            sims.append(cosine_scores[0][0].item())\n",
    "        # Store the similarities by id topic\n",
    "        d = {\"id\":range(0,20),\"cosine_sim\":sims}\n",
    "        df = pd.DataFrame(data=d)\n",
    "        # Save the results in a json file\n",
    "        df.to_json(sim_path+q+\"_topics_results_\"+str(i)+\"_\"+emb_model+\"_\"+c+\".json\")\n",
    "        # Save the results for each combination of conv. model and num. of words in a 'results' dataframe\n",
    "        results[str(i)+\"_\"+c] = sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b. Show the similarity results obtained and save the results in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>4_bart-tl-all</th>\n",
       "      <th>4_bart-tl-ng</th>\n",
       "      <th>10_bart-tl-all</th>\n",
       "      <th>10_bart-tl-ng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sport hockey</td>\n",
       "      <td>0.455012</td>\n",
       "      <td>0.455012</td>\n",
       "      <td>0.053746</td>\n",
       "      <td>0.210210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>religion atheism</td>\n",
       "      <td>0.586220</td>\n",
       "      <td>0.191222</td>\n",
       "      <td>0.536015</td>\n",
       "      <td>0.136997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>science space</td>\n",
       "      <td>0.350002</td>\n",
       "      <td>0.350003</td>\n",
       "      <td>0.350003</td>\n",
       "      <td>0.350003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>science medicine</td>\n",
       "      <td>0.252070</td>\n",
       "      <td>0.190956</td>\n",
       "      <td>0.168083</td>\n",
       "      <td>0.279395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politics_misc</td>\n",
       "      <td>0.226567</td>\n",
       "      <td>0.226567</td>\n",
       "      <td>0.331462</td>\n",
       "      <td>0.185002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>computer mac hardware</td>\n",
       "      <td>0.433676</td>\n",
       "      <td>0.387201</td>\n",
       "      <td>0.387201</td>\n",
       "      <td>0.387201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>politics mideast</td>\n",
       "      <td>0.261565</td>\n",
       "      <td>0.113921</td>\n",
       "      <td>0.362944</td>\n",
       "      <td>0.113921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>computer ibm hardware</td>\n",
       "      <td>0.436425</td>\n",
       "      <td>0.423988</td>\n",
       "      <td>0.329375</td>\n",
       "      <td>0.423988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>for sale</td>\n",
       "      <td>0.162414</td>\n",
       "      <td>0.404935</td>\n",
       "      <td>0.404935</td>\n",
       "      <td>0.404935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>science electronics</td>\n",
       "      <td>0.055412</td>\n",
       "      <td>0.312479</td>\n",
       "      <td>0.162447</td>\n",
       "      <td>0.323538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>computer windows misc</td>\n",
       "      <td>0.342185</td>\n",
       "      <td>0.485278</td>\n",
       "      <td>0.214941</td>\n",
       "      <td>0.214941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>motor motorcycle</td>\n",
       "      <td>0.904174</td>\n",
       "      <td>0.904174</td>\n",
       "      <td>0.076015</td>\n",
       "      <td>0.076015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sport baseball</td>\n",
       "      <td>0.459002</td>\n",
       "      <td>0.459002</td>\n",
       "      <td>0.459002</td>\n",
       "      <td>0.187961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>religion christian</td>\n",
       "      <td>0.829337</td>\n",
       "      <td>0.829337</td>\n",
       "      <td>0.474334</td>\n",
       "      <td>0.829337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>politics guns</td>\n",
       "      <td>0.186532</td>\n",
       "      <td>0.186532</td>\n",
       "      <td>0.186532</td>\n",
       "      <td>0.333961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>computer graphics</td>\n",
       "      <td>0.227405</td>\n",
       "      <td>0.227405</td>\n",
       "      <td>0.579564</td>\n",
       "      <td>0.236793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>motor autos</td>\n",
       "      <td>0.088624</td>\n",
       "      <td>0.579330</td>\n",
       "      <td>0.195199</td>\n",
       "      <td>0.195199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>religion misc</td>\n",
       "      <td>0.639544</td>\n",
       "      <td>0.639544</td>\n",
       "      <td>0.639544</td>\n",
       "      <td>0.639544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>computer windows x</td>\n",
       "      <td>0.383527</td>\n",
       "      <td>0.383527</td>\n",
       "      <td>0.286654</td>\n",
       "      <td>0.181907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>science crypt</td>\n",
       "      <td>0.361185</td>\n",
       "      <td>0.145967</td>\n",
       "      <td>0.021636</td>\n",
       "      <td>0.192911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   topics  4_bart-tl-all  4_bart-tl-ng  10_bart-tl-all  \\\n",
       "0            sport hockey       0.455012      0.455012        0.053746   \n",
       "1        religion atheism       0.586220      0.191222        0.536015   \n",
       "2           science space       0.350002      0.350003        0.350003   \n",
       "3        science medicine       0.252070      0.190956        0.168083   \n",
       "4           politics_misc       0.226567      0.226567        0.331462   \n",
       "5   computer mac hardware       0.433676      0.387201        0.387201   \n",
       "6        politics mideast       0.261565      0.113921        0.362944   \n",
       "7   computer ibm hardware       0.436425      0.423988        0.329375   \n",
       "8                for sale       0.162414      0.404935        0.404935   \n",
       "9     science electronics       0.055412      0.312479        0.162447   \n",
       "10  computer windows misc       0.342185      0.485278        0.214941   \n",
       "11       motor motorcycle       0.904174      0.904174        0.076015   \n",
       "12         sport baseball       0.459002      0.459002        0.459002   \n",
       "13     religion christian       0.829337      0.829337        0.474334   \n",
       "14          politics guns       0.186532      0.186532        0.186532   \n",
       "15      computer graphics       0.227405      0.227405        0.579564   \n",
       "16            motor autos       0.088624      0.579330        0.195199   \n",
       "17          religion misc       0.639544      0.639544        0.639544   \n",
       "18     computer windows x       0.383527      0.383527        0.286654   \n",
       "19          science crypt       0.361185      0.145967        0.021636   \n",
       "\n",
       "    10_bart-tl-ng  \n",
       "0        0.210210  \n",
       "1        0.136997  \n",
       "2        0.350003  \n",
       "3        0.279395  \n",
       "4        0.185002  \n",
       "5        0.387201  \n",
       "6        0.113921  \n",
       "7        0.423988  \n",
       "8        0.404935  \n",
       "9        0.323538  \n",
       "10       0.214941  \n",
       "11       0.076015  \n",
       "12       0.187961  \n",
       "13       0.829337  \n",
       "14       0.333961  \n",
       "15       0.236793  \n",
       "16       0.195199  \n",
       "17       0.639544  \n",
       "18       0.181907  \n",
       "19       0.192911  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the results as an xlsx file\n",
    "results.to_excel(\"Bart-TL/Bart_Evaluation_\"+emb_model+\".xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA SIMILARITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3a. For the conversational models, QA models, embedding model, question structure and number of topic words selected, generate the similarity values between the topic ground truth embedding and the QA answer embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of conversational models\n",
    "conv_models = [\"blenderbot-400M-distill\",\"DialoGPT-medium\",\"ChatGPT\"]\n",
    "\n",
    "# Define the name of QA models\n",
    "#qa_models = [\"deberta-v3-large-squad2\",\"deberta-v3-base-squad2\", \"xlm-roberta-large-squad2\",\\\n",
    "#              \"bert-large-uncased-whole-word-masking-squad2\", \"roberta-base-squad2-distilled\"]\n",
    "qa_models = [\"deberta-v3-base-squad2\", \"bert-large-uncased-whole-word-masking-squad2\", \"roberta-base-squad2-distilled\"]\n",
    "\n",
    "# Define the embedding model name\n",
    "#emb_model = \"all-mpnet-base-v2\"\n",
    "emb_model = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Define the path where the embeddings of the topics are stored\n",
    "emb_topic_path = \"Eval/QAEmbeddings/\"+emb_model+\"/topics/\"\n",
    "\n",
    "# Define the name of the question structure used\n",
    "q = \"q3\"\n",
    "\n",
    "# Define the number of words to be used\n",
    "num = [4,9,10]\n",
    "# For each QA model\n",
    "for qa in qa_models:\n",
    "    # For each number of words\n",
    "    for i in num:\n",
    "        # Define the path where the embeddings of the answers are stored\n",
    "        emb_path = \"Eval/QAEmbeddings/\"+emb_model+\"/\"\n",
    "        # Define the path where the similarities are to be stored\n",
    "        sim_path = \"Eval/QASimilarities/\"\n",
    "        \n",
    "        # For each conversational model\n",
    "        for c in conv_models:\n",
    "             # Array to save the similarities\n",
    "            sims = []\n",
    "            # Define the prefix name of the similarities' files of the conversational model c, QA model qa with i words\n",
    "            prefix = \"tensor_\"+q+\"_topics_results_\"+str(i)+\"_\"+c+\"_\"+qa+\"_\"\n",
    "            # For each topic (we have 20 topics)\n",
    "            for j in range(0,20):\n",
    "                # Define the path of the file where the QA answer tensor for topic j is\n",
    "                t_answ_name = emb_path+prefix+str(j)+\".pt\"\n",
    "                 # Define the path of the file where the groud truth tensor for topic j is\n",
    "                t_topic_name = emb_topic_path+\"tensor_topic_\"+str(j)+\".pt\"\n",
    "                # Load the conv. answer and GT tensors\n",
    "                emb_answ = torch.load(t_answ_name)\n",
    "                emb_topic = torch.load(t_topic_name)\n",
    "\n",
    "                # Compute similarity scores of the two embeddings/tensors\n",
    "                cosine_scores = util.pytorch_cos_sim(emb_topic, emb_answ)\n",
    "                # Save the similarity value\n",
    "                sims.append(cosine_scores[0][0].item())\n",
    "\n",
    "            # Store the similarities by id topic\n",
    "            d = {\"id\":range(0,20),\"cosine_sim\":sims}\n",
    "            df = pd.DataFrame(data=d)\n",
    "             # Save the results in a json file\n",
    "            df.to_json(sim_path+q+\"_topics_results_\"+str(i)+\"_\"+emb_model+\"_\"+c+\"_\"+qa+\".json\")\n",
    "             # Save the results for each combination of conv. model, QA model and num. of words in a 'results' dataframe\n",
    "            results[str(i)+\"_\"+c+\"_\"+qa] = sims\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b. Show the similarity results obtained and save the results in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the results as an xlsx file\n",
    "results.to_excel(\"Eval/QA_Evaluation_\"+emb_model+\".xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
